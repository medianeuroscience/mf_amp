---
title: "Language & Emotion | Preliminary Analysis"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r importing packages}
library(knitr)
library(tidyverse)
library(gridExtra)
library(ggridges)
```


```{r importing data and defining functions}
surveydata = read.csv("data/surveydata.csv")
files = list.files(path ="data", pattern="^10.*.csv$", full.names = TRUE)
expdata_full = do.call(bind_rows, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))

expdata <- expdata_full %>% 
  select(participant, 
         words, 
         mask, 
         corr_ans, 
         category,
         nonwords, 
         tweets, 
         topic, 
         task, 
         keypress, 
         RT, 
         corr, 
         participant, 
         AMP_nonword_rating.response,	
         AMP_word_rating.response,	
         AMP_random_rating.response,	
         LDT_rating.response,
         AMP_loop.thisN,
         AMP_trials.thisN)

# This function will check to see whether an observation is an outlier based on median absolute deviation (MAD)

out_mad <- function(x, thres = 3, na.rm = TRUE) {
  abs(x - median(x, na.rm = na.rm)) >= thres * mad(x, na.rm = na.rm)
}
```

# Recoding and Data Checks

Let's do some recoding and variable manipulation. 

- For some reason PsychoPy records correct as 0 and incorrect as 1, so let's switch those. 
- We'll add a new variable caled "wordcat" that tells us whether the word is moral, nonmoral, or a nonword.
- We'll split up the category variable into "foundation" and "valence" in case we are interested in the two things separately at any point.
- Looks like I accidentally named the AMP different things in condition 1 and condition 2. To remedy this, we'll coalesce the 'AMP_loop.thisN` and `AMP_trials.thisN` variables together into one column.

```{r}
expdata <- expdata %>% mutate(corr = ifelse(corr == 1, 0,
                                            ifelse(corr == 0, 1, NA)),
                              wordcat = ifelse(category == "neutral", "neutral",
                                               ifelse(category == "nonword", "nonword",
                                                      ifelse(!is.na(category), "moralword", NA))))

expdata <- expdata %>% separate(category, c("foundation", "valence"), "[.]") %>% 
  mutate(valence = ifelse(foundation == "control" & words == "pleasant", "positive",
                          ifelse(foundation == "control" & words == "unpleasant", "negative", 
                                 ifelse(foundation == "nonword", "nonword",
                                        ifelse(valence == "virtue", "positive",
                                               ifelse(valence == "vice", "negative", NA))))))

expdata <- expdata %>% mutate(trialnum = coalesce(AMP_loop.thisN, AMP_trials.thisN))

```

First, let's make sure that we have all of the data and that we don't have duplicate subject numbers. For this we need to get the LDT trials and the AMP trials specifically, and then check to see how many trials we have per subject.

```{r}
AMP_trials <- expdata %>% filter(task == "AMP")
LDT_trials <- expdata %>% filter(task == "LDT")

AMP_trials %>% group_by(participant) %>% summarize(n = n())
LDT_trials %>% group_by(participant) %>% summarize(n = n())
```

Looks like it's all there. 

Now we need to do a little bit of cleaning to try our best to get rid of outlying STRTs as well as those who didn't actually try in the task(s). I'm going to look at a few things: a) who they self-reported random responses, b) chance accuracy in the LDT, c) distributions of response times, d) the participants that the RA's called out as not paying attention

First, lets look at self-reported ratings. These are going to be fairly interesting. 

```{r rating responses}

df <- expdata %>% 
  select(participant, AMP_nonword_rating.response,	AMP_word_rating.response,	AMP_random_rating.response,	LDT_rating.response) %>% 
  group_by(participant) %>%
  summarize(AMP_word = mean(AMP_word_rating.response, na.rm=TRUE), 
            AMP_nonword = mean(AMP_nonword_rating.response, na.rm=TRUE), 
            AMP_random = mean(AMP_random_rating.response, na.rm=TRUE), 
            LDT_random = mean(LDT_rating.response, na.rm=TRUE))

plot_data <- df %>% gather("task", "response", -participant)

ggplot(plot_data, aes(x = response, y = ..count.., fill = task)) + 
  geom_bar(position = "dodge") + 
  scale_x_continuous(breaks=1:7,labels=c("not at all","","","somewhat","","","completely")) + 
  scale_fill_discrete(labels=c("AMP judgment based on targets","AMP judgment based on primes","AMP responded randomly", "LDT responded randomly")) + 
  labs(title = "Post-task Responses")

df %>% select(participant, LDT_random, AMP_random) %>% filter(AMP_random == 7 | LDT_random == 7)

```

Looks like people were more willing to admit they responded randomly in the LDT (1001,1013,1023,1029,1032,1063) than the AMP (1032, 1038, 1080). Let's look at button presses in the AMP and the LDT.

```{r press proportions}

df1 <- expdata %>% filter(task == "LDT" | task == "AMP") %>% group_by(participant, task, keypress) %>%  summarize(n = n())

df1 <- df1 %>% spread(keypress, n) %>% summarize(ratio = left/right)

median(df1$ratio) + 3 * sd(df1$ratio)

```

There are a few clear outliers here. 1031 and 1074. 1074 was one of the ones called out by the RA's as not trying at all. Would probably be worth filtering if they pop up again. 

```{r LDT accuracy}

LDT_trials <- expdata %>% 
  group_by(participant) %>% 
  mutate(LDT_random = mean(LDT_rating.response, na.rm=TRUE)) %>%
  ungroup() %>%
  filter(task == "LDT") %>%
  group_by(participant) %>% 
  summarize(corr = mean(corr, na.rm=TRUE))
  
#Let's see if there are any participants who are correctness outliers.

df_indouts <- LDT_trials %>% 
  select(participant, corr) %>% 
  mutate_if(is.numeric, funs(mad = out_mad))

table(df_indouts$corr_mad)

#Doesn't look like it. Let's look at this based on words. Should be interesting but I don't know if we have enough data for strong conclusions here yet. 

LDT_trials <- expdata %>% 
  group_by(words) %>% 
  filter(task == "LDT") %>%
  summarize(corr = mean(corr, na.rm=TRUE))

df <- LDT_trials %>% group_by(words) %>% summarize(corr = mean(corr), n = n())

df_wordouts <- df %>% 
  select(words, corr, n) %>% 
  mutate_if(is.numeric, funs(mad = out_mad))

table(df_wordouts$corr_mad)

# Only two words are outliers, the non-word "ammessment," and the non-word "tove" Only 28% of people got ammessment and only 27% got tove correct.

```

Moral words were more accurately responded to as words than were nonwords. Looks like there are a few people that hover around chance in their responses but they aren't technically outliers. 

Now let's looks at reaction times in the AMP and the LDT. I'm not sure that we are going to be able to pick out people who are just clicking through based on reaction time, but I'll do some cross referencing with the people who the RA's said were just clicking through without looking at the screen and see if I can get an idea. 

```{r reaction times}

df <- expdata %>% filter(task == "AMP" | task == "LDT")

ggplot(df, aes(x = RT)) + geom_histogram()

# Pretty highly right-skewed. Let's remove outliers within subjects and conditions and then check to see how we look.

df_indouts <- df %>% 
  group_by(participant, task) %>% 
  select(participant, 
         RT, 
         foundation, 
         valence, 
         wordcat, 
         task, 
         words, 
         corr) %>% 
  mutate_if(is.numeric, funs(mad = out_mad))

# Filtering based on that column
expdata_rtfilt <- filter(df_indouts, RT_mad == FALSE) 

ggplot(expdata_rtfilt, aes(x = RT)) + geom_histogram()

# Looks a little better. Let's see if we have any partipant outliers. Here I'll go back to the unfiltered dataset and run the same procedure on the participant means.

df_groupouts <- df %>% 
  group_by(participant, task) %>%
  summarize(meanrt = mean(RT, na.rm=TRUE)) %>%
  ungroup() %>%
  group_by(task) %>%
  select(participant, 
         task, 
         meanrt) %>% 
  mutate_if(is.numeric, funs(mad = out_mad))

# Looks like we have a few outliers, but I'm hestitant to toss them since they're on the long end, suggesting that they are thinking about their answers rather than just thumbing through mindlessly. We can do some thinking on this.
```

So the only cleaning that that we've done so far is re: reaction times. We can perhaps do some more thoughtful cleaning in the future. Now that we have filtered, we can go on to some some preliminary analysis. Let's do some EDA plotting

# LDT EDA

```{r}
# First, let's look at neutral words vs. nonwords vs. moral words

plt_data <- expdata_rtfilt %>% 
  filter(task == "LDT") %>%
  group_by(participant, wordcat) %>% 
  summarize(corr = mean(corr), RT = mean(RT))

# Box plot of correctness by category
plt1 <- ggplot(plt_data, aes(x = wordcat, y = corr, fill = wordcat)) + geom_boxplot() + coord_flip()

# Density plot of correctness by category
plt2 <- ggplot(plt_data, aes(x = corr, fill = wordcat, color = wordcat)) + geom_density(alpha = .8)

# Boxplot of RT by category
plt3 <- ggplot(plt_data, aes(x = wordcat, y = RT, fill = wordcat)) + geom_boxplot() + coord_flip()

#Density plot of RT by category
plt4 <- ggplot(plt_data, aes(x = RT, fill = wordcat, color = wordcat)) + geom_density(alpha = .8)

grid.arrange(plt1, plt2, plt3, plt4)

# Importing the weights to see how they correlate with response times and accuracy.

weights = read.csv("data/mfd_weights.csv")
weights <- weights %>% select(-X) %>% rename(words = word)

LDT_words <- expdata_rtfilt %>% 
  filter(task == "LDT" & wordcat == "moralword") %>% 
  group_by(words) %>% 
  summarize(meancorr = mean(corr), 
            meanrt = mean(RT), 
            cat = paste(unique(foundation), collapse = ', '), 
            val = paste(unique(valence), collapse = ', '))

LDT_weighted <- left_join(LDT_words, weights, by = "words")

plot_data <- filter(LDT_weighted, !is.na(weight))

ggplot(plot_data, aes(x = weight, y = meancorr)) + 
  geom_point() + 
  geom_smooth(method='lm',formula=y~x, se=F)

ggplot(plot_data, aes(x = weight, y = meanrt)) + 
  geom_point() + 
  geom_smooth(method='lm',formula=y~x, se=F)

# Looks like nothing really going on here.

```

# AMP EDA

Before we do anything with the AMP, we need to manipulate the data in a way that it is able to be analyzed (turning it from wide to long).

```{r AMP gathering}

AMP_trials <- AMP_trials %>% 
  mutate(subnum = participant) %>% 
  unite("sub_trial", "subnum", "trialnum", sep = "_")

AMP_trials_long <- AMP_trials %>% 
  rename("prime_cat" = "wordcat",
         "prime_foundation" = "foundation",
         "prime_val" = "valence") %>%
  gather("prime_target", "word", "words", "nonwords") %>%
  mutate(prime_target = ifelse(prime_target == "words", "prime",
                               ifelse(prime_target == "nonwords", "target", NA)),
         val_response = ifelse(keypress == "left", -1, 1),
         prime_val = ifelse(is.na(prime_val), "nonword", prime_val)) %>%
  arrange(by = sub_trial) %>%
  select(-c(LDT_rating.response, AMP_loop.thisN, AMP_trials.thisN, tweets, topic))

```

Okay now we can do some investigating. I'm going to look at the following:

- Plotting differences in responses between moral primes and nonword primes
- Whether the valence of primes leads to differences in responses (valence or speed)
- Whether the weighting of the word in the dictionary is associated with differences in response times

There's a discussion to be had about whether these plots should be done at the word level (each word is an observation within each category) or at the participant level (each participant is one observation within each category). Each gives us different views of the data.

I still haven't incorporated the survey data but I'll do that in the next section.

```{r}
plot_data <- AMP_trials_long %>% group_by(word, prime_foundation, prime_val) %>% filter(prime_target == "prime") %>% summarize(mean_val_response = mean(val_response), meanrt = mean(RT))

p1 <- ggplot(plot_data, aes(x = mean_val_response, fill = prime_val, color = prime_val)) + geom_density(alpha = .8) + labs(title = "Mean valence of responses by prime category", x = "Valence", y = "Density")

p2 <- ggplot(plot_data, aes(x = meanrt, fill = prime_val, color = prime_val)) + geom_density(alpha = .8) + labs(title = "Mean RT of responses by prime category", x = "RT", y = "Density")

grid.arrange(p1, p2, ncol = 2)
```

Interesting! Looks like the procedure works, but need to test statistically to be sure. Vice words elicit negative valence, virtue words elicit positive valence, and nonwords are somewhere in the middle. Nothing really interesting for RTs. Let's look at the foundations and the weights. 

```{r}
plot_data <- AMP_trials_long %>% group_by(word, prime_foundation, prime_val) %>% filter(prime_target == "prime") %>% summarize(mean_val_response = mean(val_response), meanrt = mean(RT), n = n())

ggplot(plot_data, aes(x = mean_val_response, y = prime_foundation, fill = prime_val, color = prime_val)) +
  geom_density_ridges(alpha = .8, scale = .9, jittered_points = TRUE, position = "raincloud") +
  scale_fill_manual(values=c("#FF6C67", "#00B2FD", "#00BE0D")) + 
  scale_color_manual(values=c("#FF6C67", "#00B2FD", "#00BE0D")) +
  labs(title = "Mean valence of responses by foundation", x = "Valence", y = "Density")
```

Very neat. Looks like our valences line out quite nicely with the words in the moral categories. Note that the controls aren't plotting because there is only one observation per category (the word "pleasant" and the word "unpleasant"). The mean for "pleasant is $.201$ and the mean for "unpleasant" is $-.34$ so they align roughly as expected. Some of the nonword primes have low numbers of observations ($n = 4$) for some, but as we recruit more participants this should even out.

```{r}

```

